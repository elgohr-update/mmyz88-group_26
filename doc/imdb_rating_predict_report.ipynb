{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Predicting IMDB Ratings Based on Movie Reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Yuanzhe Marco Ma, Arash Shamseddini, Kaicheng Tan, Zhenrui Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Project Overview](#1)\n",
    "- [Data](#2)\n",
    "- [Exploratory Data Analysis (EDA)](#3)\n",
    "- [Prediction](#4)\n",
    "    - [Preprocessing](#4)\n",
    "    - [Model Fitting](#5)\n",
    "    - [Results](#6)\n",
    "- [Reflection](#7)\n",
    "- [References](#8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Project Overview <a class=\"anchor\" id=\"1\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we look at the relationship between movie reviews and their IMDB scores (ranging from 0 ~ 10). Positive reviews are often related to high IMDB scores, while negative reviews indicate the opposite. While it is easy for humans to understand a piece of review and guess the scores, we wonder if machines could understand it as well. Furthermore, we would like to automate this process, so that given a bunch of movie reviews, we are able to predict their corresponding IMDB scores easily. <br>\n",
    "\n",
    "In this project, most of the data wrangling in this project is powered by the [pandas](#c4) (McKinney 2010) library. The machine learning models and frameworks utilize [sklearn](#c3) (Pedregosa et al. 2011). We use Support Vector Regressor (SVR) as our prediction model. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Where Do We Get the Data? <a class=\"anchor\" id=\"2\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained our data from an open-sourced github repository: <br> \n",
    "> https://github.com/nproellochs/SentimentDictionaries/blob/master/Dataset_IMDB.csv <br>\n",
    "\n",
    "The repository was originally used for sentiment analysis related to movie reviews. Here we are using the `Dataset_IMDB.csv` as our main data source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automation\n",
    "To automate data retrieval, we have written a script to obtain the dataset with Python. The script can be accessed [here](../src/download_data.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. What does the data look like? <a class=\"anchor\" id=\"3\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the dataset by performing some **Exploratory Data Analysis (EDA)**. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Columns:\n",
    "| Column Name | Column Type | Description                             |\n",
    "|-------------|-------------|-----------------------------------------|\n",
    "| Id          | Numeric     | Unique ID assigned to each observation. |\n",
    "| Text        | Free Text   | Body of the review content.             |\n",
    "| Author      | Categorical | Author's name of the review             |\n",
    "| Rating      | Numeric     | Ratings given along with the review (normalized)   |\n",
    "\n",
    "For this project, we look primarily into the `Text` and `Rating` columns.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We realized that `Author` may have significant relation to ratings, but since we are making a generalized model for reviews from any audience, we have decided to discard it for this analysis. Therefore, we will **drop** both the `Author` and `Id` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The `Text` feature\n",
    "The `Text` feature contains all the movie reviews. This will be our primarily input feature. <br>\n",
    "Below are the top 10 most frequent words in the reviews:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Word   | Frequency | Rank |\n",
    "|--------|-----------|------|\n",
    "|the     |   172557  |  1   |\n",
    "|of      |   78038   |  2   |\n",
    "|and     |   76392   |  3   |\n",
    "|to      |   74239   |  4   |\n",
    "|is      |   57547   |  5   |\n",
    "|in      |   49646   |  6   |\n",
    "|that    |   33476   |  7   |\n",
    "|it      |   33061   |  8   |\n",
    "|as      |   27536   |  9   |\n",
    "|with    |   26852   |  10  |\n",
    "\n",
    "As we can see, the most frequent words are often generic words such as prepositions and pronouns, which has little implication to our model learning. We might want to avoid overfitting to these words as we train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The `Rating` Class\n",
    "`Ratings` will be our target class. Let's look at a distribution of `Rating`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ratings_histo](../results/histogram_rating_distribution.svg)\n",
    "\n",
    "The ratings seem roughly normally distributed, with a little skewness to the left. Most of the ratings cluster around 0.5 ~ 0.8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correlation between `Text` length and `Rating`\n",
    "We suspect that people more passionate about certain movies tend to write longer reviews to express feelings. This could also be true for very negative reviews. <br> <br>\n",
    "A bar plot of `Text` length vs `Rating` is presented below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textlength_vs_rating](../results/histogram_rating_vs_text_length.svg)\n",
    "\n",
    "There doesn't seem to be a strong correlation between reviews length and rating. However, it is notable that for the most positive ratings (from 0.7 ~ 1.0), the reviews tend to be higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Pre-processing <a class=\"anchor\" id=\"4\"></a>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Columns\n",
    "\n",
    "In addition to `Text` feature alone, we extracted two potentially useful columns that could enhance our machine learning model. \n",
    "\n",
    "#### 1. `n_words`\n",
    "As mentioned above, we suspect some correlation between review lengths and ratings. Therefore we created an `n_words` feature, which counts the number of words in each review.  \n",
    "#### 2. `sentiment`\n",
    "We utilized the [**NLTK**](#c2) (Bird 2009) package to assist us in extracting the sentiment of each review. This `sentiment` feature will have four ordinal categories - ['neg', 'compound', 'neu', 'pos']. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Transformer\n",
    "\n",
    "Now we have three main columns to transform before fitting - `Review`, `n_words` and `sentiment`. \n",
    "- `Review`: since it is text feature, we will use sklearn's `CountVectorizer` to transform the text into bag of words. As mentioned in the EDA, since the most frequent words are often meaningless, we added `stop_words=20` argument into the vectorizer to exclude the top 20 most frequent words.\n",
    "- `n_words`: we standardize the data with sklearn's `StandardScaler` to avoid its scaling effect on the models. \n",
    "- `sentiment`: since it is ordinal data, we utilize sklearn's `OrdinalEncoder` to encode the ordinal data in one-hot style. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V. Model Selection & Fitting <a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training data ready, we will consider which model to use to fit. <br>\n",
    "We selected 3 candidate models that are suitable for regression prediction:\n",
    "1. Support Vector Regressor (SVR)\n",
    "2. Random Forest Regressor\n",
    "3. Ridge Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tuned the hyperparameters of each model using sklearn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). <br>\n",
    "Below are comparative cross-validation scores of the 3 models.\n",
    "\n",
    "| model                 | fit_time   | score_time | test_score   |  train_score |\n",
    "|-----------------------|------------|------------|--------------|--------------|\n",
    "| Ridge                 | 2.198422   | 0.511951   | 0.532790     |  0.799778    |\n",
    "| SVR                   | 28.988741  | 8.086322   | 0.469965     |  0.723018    |\n",
    "| RandomForestRegressor | 287.661862 | 0.499031   | 0.359660     |  0.906179    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Winner: `SVR`\n",
    "\n",
    "We selected `SVR` with a combindation of two considerations - cross-validation scores and the risk of overfitting. `SVR` was selected because it has a CV score comparable to the `Ridge` regressor, while having the least amount of overfitting. For more details on the model selection process, see [this report](../doc/model_selection.ipynb).\n",
    "\n",
    "Below is a detailed specification of our final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model Name | Hyperparameter - Gamma | Mean Fit Time |  Mean Scoring Time | Mean CV Score |\n",
    "|------------|------------------------|---------------|--------------------|-------------|\n",
    "|    SVR     |  0.0007 | 28.988741s | 8.086322s |0.469965 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI. Prediction Results <a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our model's prediction scores on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r2</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.50371</td>\n",
       "      <td>1.286077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            r2      rmse\n",
       "model  0.50371  1.286077"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "scores = pd.read_csv('../results/model_test_scores.csv', index_col=0)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1. **$R^2$**, RMSE Scores\n",
    "\n",
    "$R^2$ and RMSE (Root-mean-squared-error) are two common metrics for evaluating a regression model's accuracy. <br> \n",
    "\n",
    "The obtained $R^2$ score for the test set is **0.5037**, which was comparable to, and even better than our validation score. The RMSE score was **1.2861**. This seems large because it means our predicted score can have a margin of error of over 1.2 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prediction vs. True Ratings\n",
    "We have also created a scatterplot to compare our predicted ratings vs. the true ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![true_vs_predict](../results/true_vs_predict.svg)\n",
    "\n",
    "> There is an obvious difference between the predicted ratings and true ratings. In the true ratings, people tend to give whole number rating, i.e. 0.3 instead of 0.3247. Our model did not capture that. \n",
    "\n",
    "Despite that, most of the points are somewhat clustered around the identity line. This indicates that our model didn't seem to under-fit or over-fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII. Reflection and Improvements <a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with our prediction and analysis, we can examine the quality of our work. \n",
    "In fact, there are a few areas of improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We discarded the `Author` and `id` columns at the beginning. In fact, these columns may be influential features, especially `Author`. One major flaw of our dataset is that all reviews come from four critics. This makes it difficult to generalize to the broader audience. It would better if we can obtain reviews from the general audiences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is risk of overfitting in our models. Based on the model selection results, all three candidate models have large gaps between training score and test scores. We have tuned the hyperparameters the best we can, yet the gap is still apparent. This might have to do with the nature of our dataset and model compatibility, and it requires further analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As mentioned above, our model did not capture the pattern where humans tend to give whole number scores. In the future, we can probably give more emphasis to predicting whole number scores, so that our model resembles more human behaviors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `sentiment` feature genereated with the NLTK  package contained only `neu` and `compound` in our dataset. This is confusing and we have yet to understand this behavior. We included the feature regardless because it may still provide useful information. However, this is definitely a place we need to investigate further into. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "1. Dua, Dheeru, and Casey Graff. 2017. “UCI Machine Learning Repository.” University of California, Irvine, School of Information; Computer Sciences. (http://archive.ics.uci.edu/ml) <a class=\"anchor\" id=\"c1\"></a>\n",
    "2. Bird, Steven, Edward Loper and Ewan Klein (2009). Natural Language Processing with Python.  O'Reilly Media Inc. (https://www.nltk.org/) <a class=\"anchor\" id=\"c2\"></a>\n",
    "3. Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011. (https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html) <a class=\"anchor\" id=\"c3\"></a>\n",
    "4. McKinney, W., & others. (2010). Data structures for statistical computing in python. In Proceedings of the 9th Python in Science Conference (Vol. 445, pp. 51–56). (https://pandas.pydata.org/)  <a class=\"anchor\" id=\"c4\"></a>\n",
    "4. Gareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani, 2009, “An Introduction to Statistical Learning with Application in R”, Springer Publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
